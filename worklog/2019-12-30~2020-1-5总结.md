---
title: 2019-12-30~2020-1-5总结
tags: AlgorithmLearning, WorkLog
renderNumberedHeading: true
grammar_cjkRuby: true
---

今天就简要总结一下上周学了什么东西吧.

所有的学习都是基于以下方面:压缩和加速深度学习神经网络模型.
首先学习了主流的压缩加速方法(剪枝,参数分享,量化,知识蒸馏)等等,也找寻了他们和NAS等相关领域的一些应用.但是,此时我对NAS的了解程度也还不够深刻.以后估计还会花费时间继续对这方面进行学习的.

除此之外,按照王老师的要求,我重点学习了reformer这篇论文,就是讨论的如何将LSH应用于transformer模型从而实现对transformer模型的加速和压缩.这个方法主要是在squencelength比较长的时候效果才比较好，所以对于常见的场景而言并不是十分有优势。当然，审稿人没有意识到这一点哈哈。

整个总结的ppt被我汇聚如下：
[here](https://github.com/liangzid/NoteBook/blob/master/KDD/ziliang-Accelarate%20and%20Compression%20DNN%20with%20hash.pptx)
