---
title: LSH的学习 
tags: AlgorithmLearning, WorkLog, LSH
renderNumberedHeading: true
grammar_cjkRuby: true
---
前几天做实验,similaritySearch,用到了LSH算法,向李润东学长请教了一下,记录一下吧.
首先,该算法应用的典范是 _最近邻_ 问题.所谓最近邻是指对于N个不同大小的集合,如何去判断其中某个集合和其他集合里的那个(或那些)最近? 一种特殊的形不似神似的理解是:在一个两两节点联通的图中(节点之间的距离代表他们的相似度),如何去寻找特定节点B使得它离节点A最近.

针对上述问题,最直接的思路是计算所有集合到目标集合的相似度,这样对于N个集合进行k次搜索就需要N\*k的次数,这是无法接受的,当每个集合维度很高时更是不可思议.在这个背景下,局部敏感哈希(Local Sensitive Hashing)就产生了.

大致想法是: 对于长度不同的若干集合,首先生成他们的sketch(这或许是李润东学长说sketch就是一个副产品的原因),之后,基于该sketch,构造一种特殊的hash(或者说映射,或者说对应关系),这种映射也可以用python里的dict结构理解.这种hash的key基于sketch生成,value则是对应的集合label.(至于如何选取后面将会详述.)经过这种方法,就可以实行我们的LSH了,即:对于集合A的字典dictA,将其key放入剩余所有集合字典的并集里,查看是否能够返回若干个集合label,最终将所有label取并集就可以得到和A最相似的若干个集合.

这里面令人疑惑的地方是:为什么是若干个集合,不是最相似的么?

这就是LSH的特点了.上述算法可以实现(也是LSH的标准定义)的目标是,在统计意义上,让相似度超过某个阈值p的若干个集合有极大概率会和搜索集合Hash到同一个bucket里,对相似度小于某个阈值q的集合有极大概率不会hash到同一个bucket里.
那么问题又来了,这个p和q如何调整得到呢?

这就是之前忽略的细节,实际上这两个参数是通过设定key以及sketch的长度来实现的.为了方便理解,可以举一个极端的例子:
假设对于长度是k的sketch,我们使用该sketch中的每一位作为键值,那么,对于两个集合A,B的sketch skA和skB,出现任意第i位相等(也就是可以碰撞到一个桶里)的期望是A与B的jaccard相似度J(MinHash世界下),这样再第i位下无法碰撞的概率就是(1-J),这样整个sketch都无法发生碰撞亦即LSH不判定A与B相似的概率就是(1-J)^k ,这样可以发生碰撞的概率就是(1-(1-J)^k). 可以看出,当相似度很高(J很大)时,会有极高的概率将两集合映射到同一个桶里,但是这样是否就不错了呢?自然不是的!

注意到,尽管上述方案可以将相似的集合找到,但是显然会让不相似的集合也卷入到搜索结果里.原因就是相似度J是一个小于1的正数,因此在低相似度下尽管(1-J)很大但是取了k次方之后仍然是会很小的!!!这样一来最终两个不怎么相似的集合同样会被以较高的概率映射到同一个bucket里面.

这样看来,k越大越可以保证相似度超过阈值p的集合被hash到,但是与此同时阈值q的限制就被打破了.这个便是标准LSH的K-L(也称b-c)理论了.

不如让我们从另一个极端开始理解.假设,构造hash的key是整个sketch,那么可以预见:除非两个sketch完全相同,否则不可能碰撞了.我们可以计算一下概率,两个sketch每一位都相同的概率(同样是在MinHash世界下)为J^k ,也就是说两集合hash碰撞的概率为J^k ,可以说微乎其微(除非k很小,但是很小的k是没有统计意义的),而两个集合不会被碰撞的概率则是1-J^k ,也就是非常高.注意到这和之前的方案完全相反,因此将二者中和就可以实现我们的目的了,这就是b-c的想法:生成长度为b的sketch,重复进行c次实验,将结果取并集.当然也可以理解为对于长度为b\*c的sketch,分成c份用于做key,结果取并集.二者实质是一样的,这是由sketch的生成方式造成的.

还是略微算一下吧,通过K-L(b-c)方式,两个集合的得以碰撞的概率为1-(1-J^k )^L,无法碰撞的概率为(1-J^k )^L .

最后,献上学长给我讲时的图,感觉很不错.

![2](https://raw.githubusercontent.com/liangzid/LittleBook/master/小书匠/2.jpg)